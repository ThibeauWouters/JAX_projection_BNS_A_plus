Tue Apr  1 08:26:23 CEST 2025
NVIDIA H100
GPU found?
[CudaDevice(id=0)]
GPU found?
[CudaDevice(id=0)]
Ignoring the Q and Z NEP parameters
Using CSE grid with 8 points
Fixed params loaded inside the MicroToMacroTransform:
    E_sat: -16.0
Given id_list was None, so created id list [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
 25 26 27 28 29 30]
Loading the trained NF model from: models/HQC18/Aplus/1
Loaded the NF for run HQC18_1
The range of m1 for HQC18_1 is: 1.5951060503721237 to 1.9727715104818344
The range of m2 for HQC18_1 is: 1.312011256814003 to 1.6153650730848312
Loading the trained NF model from: models/HQC18/Aplus/2
Loaded the NF for run HQC18_2
The range of m1 for HQC18_2 is: 1.3507076352834702 to 1.786174550652504
The range of m2 for HQC18_2 is: 1.023673191666603 to 1.3354357331991196
Loading the trained NF model from: models/HQC18/Aplus/3
Loaded the NF for run HQC18_3
The range of m1 for HQC18_3 is: 1.6290701180696487 to 2.099888101220131
The range of m2 for HQC18_3 is: 1.2507999688386917 to 1.5923646837472916
Loading the trained NF model from: models/HQC18/Aplus/4
Loaded the NF for run HQC18_4
The range of m1 for HQC18_4 is: 1.5584642440080643 to 2.062421664595604
The range of m2 for HQC18_4 is: 0.9600573033094406 to 1.2403257936239243
Loading the trained NF model from: models/HQC18/Aplus/5
Loaded the NF for run HQC18_5
The range of m1 for HQC18_5 is: 1.67008675634861 to 2.057117149233818
The range of m2 for HQC18_5 is: 1.3671474903821945 to 1.6765356808900833
Loading the trained NF model from: models/HQC18/Aplus/6
Loaded the NF for run HQC18_6
The range of m1 for HQC18_6 is: 1.1361979693174362 to 1.4794949442148209
The range of m2 for HQC18_6 is: 0.8818090707063675 to 1.13137386739254
Loading the trained NF model from: models/HQC18/Aplus/7
Loaded the NF for run HQC18_7
The range of m1 for HQC18_7 is: 1.7870812863111496 to 2.304913178086281
The range of m2 for HQC18_7 is: 1.4346552640199661 to 1.8429436534643173
Loading the trained NF model from: models/HQC18/Aplus/8
Tried looking for the NF architecture at path /home/twouters2/projects/projection_BNS_A_plus/src/projection_BNS/NF/models/HQC18/Aplus/8.eqx, but it doesn't exist!
Could not load the likelihood for id 8, because of the following error: [Errno 2] No such file or directory: '/home/twouters2/projects/projection_BNS_A_plus/src/projection_BNS/NF/models/HQC18/Aplus/8_kwargs.json'
Moving on
Loading the trained NF model from: models/HQC18/Aplus/9
Tried looking for the NF architecture at path /home/twouters2/projects/projection_BNS_A_plus/src/projection_BNS/NF/models/HQC18/Aplus/9.eqx, but it doesn't exist!
Could not load the likelihood for id 9, because of the following error: [Errno 2] No such file or directory: '/home/twouters2/projects/projection_BNS_A_plus/src/projection_BNS/NF/models/HQC18/Aplus/9_kwargs.json'
Moving on
Loading the trained NF model from: models/HQC18/Aplus/10
Tried looking for the NF architecture at path /home/twouters2/projects/projection_BNS_A_plus/src/projection_BNS/NF/models/HQC18/Aplus/10.eqx, but it doesn't exist!
Could not load the likelihood for id 10, because of the following error: [Errno 2] No such file or directory: '/home/twouters2/projects/projection_BNS_A_plus/src/projection_BNS/NF/models/HQC18/Aplus/10_kwargs.json'
Moving on
Loading the trained NF model from: models/HQC18/Aplus/11
Loaded the NF for run HQC18_11
The range of m1 for HQC18_11 is: 1.299256756901741 to 1.62281833589077
The range of m2 for HQC18_11 is: 1.0130832344293594 to 1.2501218169927597
Loading the trained NF model from: models/HQC18/Aplus/12
Loaded the NF for run HQC18_12
The range of m1 for HQC18_12 is: 1.6495809704065323 to 1.9121313840150833
The range of m2 for HQC18_12 is: 1.456182524561882 to 1.6875570267438889
Loading the trained NF model from: models/HQC18/Aplus/13
Loaded the NF for run HQC18_13
The range of m1 for HQC18_13 is: 1.681915745139122 to 2.0141854137182236
The range of m2 for HQC18_13 is: 1.4184422045946121 to 1.6963628679513931
Loading the trained NF model from: models/HQC18/Aplus/14
Loaded the NF for run HQC18_14
The range of m1 for HQC18_14 is: 1.6363092511892319 to 2.130124792456627
The range of m2 for HQC18_14 is: 1.2622100859880447 to 1.6241564601659775
Loading the trained NF model from: models/HQC18/Aplus/15
Loaded the NF for run HQC18_15
The range of m1 for HQC18_15 is: 1.6567648202180862 to 2.0767778903245926
The range of m2 for HQC18_15 is: 1.323576644062996 to 1.6471945494413376
Loading the trained NF model from: models/HQC18/Aplus/16
Loaded the NF for run HQC18_16
The range of m1 for HQC18_16 is: 1.275683417916298 to 1.670367643237114
The range of m2 for HQC18_16 is: 0.9745586663484573 to 1.2572485953569412
Loading the trained NF model from: models/HQC18/Aplus/17
Loaded the NF for run HQC18_17
The range of m1 for HQC18_17 is: 1.0951978713274002 to 1.3199273496866226
The range of m2 for HQC18_17 is: 0.9207233041524887 to 1.1096449941396713
Loading the trained NF model from: models/HQC18/Aplus/18
Loaded the NF for run HQC18_18
The range of m1 for HQC18_18 is: 1.3384317606687546 to 1.6379211097955704
The range of m2 for HQC18_18 is: 1.1141770333051682 to 1.3447678834199905
Loading the trained NF model from: models/HQC18/Aplus/19
Loaded the NF for run HQC18_19
The range of m1 for HQC18_19 is: 1.537817046046257 to 1.8139878660440445
The range of m2 for HQC18_19 is: 1.3155234605073929 to 1.5559086948633194
Loading the trained NF model from: models/HQC18/Aplus/20
Loaded the NF for run HQC18_20
The range of m1 for HQC18_20 is: 1.2490694969892502 to 1.608438566327095
The range of m2 for HQC18_20 is: 0.9914573282003403 to 1.254277303814888
Loading the trained NF model from: models/HQC18/Aplus/21
Loaded the NF for run HQC18_21
The range of m1 for HQC18_21 is: 1.69859878718853 to 2.150139734148979
The range of m2 for HQC18_21 is: 1.0096531361341476 to 1.2563065439462662
Loading the trained NF model from: models/HQC18/Aplus/22
Loaded the NF for run HQC18_22
The range of m1 for HQC18_22 is: 1.9517839699983597 to 2.2153454273939133
The range of m2 for HQC18_22 is: 1.7249678820371628 to 1.966801956295967
Loading the trained NF model from: models/HQC18/Aplus/23
Loaded the NF for run HQC18_23
The range of m1 for HQC18_23 is: 1.3920056074857712 to 1.7409268766641617
The range of m2 for HQC18_23 is: 1.1153361946344376 to 1.3798607140779495
Loading the trained NF model from: models/HQC18/Aplus/24
Loaded the NF for run HQC18_24
The range of m1 for HQC18_24 is: 1.3898760825395584 to 1.6547567397356033
The range of m2 for HQC18_24 is: 1.189572736620903 to 1.417006179690361
Loading the trained NF model from: models/HQC18/Aplus/25
Loaded the NF for run HQC18_25
The range of m1 for HQC18_25 is: 1.2258315831422806 to 1.3944337517023087
The range of m2 for HQC18_25 is: 1.103745624423027 to 1.2540871649980545
Loading the trained NF model from: models/HQC18/Aplus/26
Loaded the NF for run HQC18_26
The range of m1 for HQC18_26 is: 1.4261224120855331 to 1.7603421956300735
The range of m2 for HQC18_26 is: 1.1617300659418106 to 1.4280133694410324
Loading the trained NF model from: models/HQC18/Aplus/27
Loaded the NF for run HQC18_27
The range of m1 for HQC18_27 is: 1.373770758509636 to 1.6603817790746689
The range of m2 for HQC18_27 is: 1.1460060626268387 to 1.390162631869316
Loading the trained NF model from: models/HQC18/Aplus/28
Loaded the NF for run HQC18_28
The range of m1 for HQC18_28 is: 1.6580089181661606 to 1.990496888756752
The range of m2 for HQC18_28 is: 1.4186658710241318 to 1.7019408196210861
Loading the trained NF model from: models/HQC18/Aplus/29
Loaded the NF for run HQC18_29
The range of m1 for HQC18_29 is: 1.3753966242074966 to 1.7970957607030869
The range of m2 for HQC18_29 is: 1.0696286708116531 to 1.3709748536348343
Loading the trained NF model from: models/HQC18/Aplus/30
Loaded the NF for run HQC18_30
The range of m1 for HQC18_30 is: 1.4894217997789383 to 2.021694555878639
The range of m2 for HQC18_30 is: 1.1030303686857224 to 1.4583279937505722
There are 27 GW likelihoods used now
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x149064a951b0>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e62040a30>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e61f8e110>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e601400a0>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e60ff9f00>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e61331d80>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e2371ac20>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e2352fc70>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e22ccac50>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e2352f910>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e2250a170>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e600ea6b0>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e22de8160>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e22976860>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e22619f90>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e21f1ffa0>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e2252f820>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e220cc730>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e210d7580>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e21602680>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e2194ea10>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e217afd00>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e21dd5630>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e2087ecb0>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e209e6dd0>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e2192de40>
<projection_BNS.EOS.inference_utils.GWlikelihood_with_masses object at 0x147e20118e50>
prior_keys
['E_sym', 'L_sym', 'K_sym', 'K_sat', 'nbreak', 'n_CSE_0_u', 'cs2_CSE_0', 'n_CSE_1_u', 'cs2_CSE_1', 'n_CSE_2_u', 'cs2_CSE_2', 'n_CSE_3_u', 'cs2_CSE_3', 'n_CSE_4_u', 'cs2_CSE_4', 'n_CSE_5_u', 'cs2_CSE_5', 'n_CSE_6_u', 'cs2_CSE_6', 'n_CSE_7_u', 'cs2_CSE_7', 'cs2_CSE_8']
all_prior_keys
['E_sym', 'L_sym', 'K_sym', 'K_sat', 'nbreak', 'n_CSE_0_u', 'cs2_CSE_0', 'n_CSE_1_u', 'cs2_CSE_1', 'n_CSE_2_u', 'cs2_CSE_2', 'n_CSE_3_u', 'cs2_CSE_3', 'n_CSE_4_u', 'cs2_CSE_4', 'n_CSE_5_u', 'cs2_CSE_5', 'n_CSE_6_u', 'cs2_CSE_6', 'n_CSE_7_u', 'cs2_CSE_7', 'cs2_CSE_8', 'key']
Prior parameter 0: E_sym
Prior parameter 1: L_sym
Prior parameter 2: K_sym
Prior parameter 3: K_sat
Prior parameter 4: nbreak
Prior parameter 5: n_CSE_0_u
Prior parameter 6: cs2_CSE_0
Prior parameter 7: n_CSE_1_u
Prior parameter 8: cs2_CSE_1
Prior parameter 9: n_CSE_2_u
Prior parameter 10: cs2_CSE_2
Prior parameter 11: n_CSE_3_u
Prior parameter 12: cs2_CSE_3
Prior parameter 13: n_CSE_4_u
Prior parameter 14: cs2_CSE_4
Prior parameter 15: n_CSE_5_u
Prior parameter 16: cs2_CSE_5
Prior parameter 17: n_CSE_6_u
Prior parameter 18: cs2_CSE_6
Prior parameter 19: n_CSE_7_u
Prior parameter 20: cs2_CSE_7
Prior parameter 21: cs2_CSE_8
Prior parameter 22: key
Fixed params loaded inside the MicroToMacroTransform:
    E_sat: -16.0
We are going to give these kwargs to Jim:
{'n_loop_training': 30, 'n_loop_production': 30, 'n_chains': 1000, 'n_local_steps': 2, 'n_global_steps': 100, 'n_epochs': 20, 'train_thinning': 1, 'output_thinning': 5}
We are going to sample the following parameters:
['E_sym', 'L_sym', 'K_sym', 'K_sat', 'nbreak', 'n_CSE_0_u', 'cs2_CSE_0', 'n_CSE_1_u', 'cs2_CSE_1', 'n_CSE_2_u', 'cs2_CSE_2', 'n_CSE_3_u', 'cs2_CSE_3', 'n_CSE_4_u', 'cs2_CSE_4', 'n_CSE_5_u', 'cs2_CSE_5', 'n_CSE_6_u', 'cs2_CSE_6', 'n_CSE_7_u', 'cs2_CSE_7', 'cs2_CSE_8', 'key']
No sample transforms provided. Using prior parameters as sampling parameters
Jim received the local sampler name: MALA
['n_dim', 'n_chains', 'n_local_steps', 'n_global_steps', 'n_loop', 'output_thinning', 'verbose']
log_prob
[-409.49397164 -334.59458858 -306.72069059]
Sampling seed is set to: 11

Global Tuning:   0%|          | 0/30 [00:00<?, ?it/s]
Global Tuning:   3%|▎         | 1/30 [07:14<3:29:53, 434.25s/it]
Global Tuning:   7%|▋         | 2/30 [08:16<1:40:35, 215.57s/it]
Global Tuning:  10%|█         | 3/30 [09:32<1:08:17, 151.74s/it]
Global Tuning:  13%|█▎        | 4/30 [10:43<51:56, 119.85s/it]  
Global Tuning:  17%|█▋        | 5/30 [11:54<42:36, 102.26s/it]
Global Tuning:  20%|██        | 6/30 [13:08<37:00, 92.54s/it] 
Global Tuning:  23%|██▎       | 7/30 [14:23<33:20, 86.99s/it]
Global Tuning:  27%|██▋       | 8/30 [15:35<30:08, 82.23s/it]
Global Tuning:  30%|███       | 9/30 [16:49<27:48, 79.45s/it]
Global Tuning:  33%|███▎      | 10/30 [18:06<26:17, 78.88s/it]
Global Tuning:  37%|███▋      | 11/30 [19:23<24:47, 78.31s/it]
Global Tuning:  40%|████      | 12/30 [20:41<23:24, 78.03s/it]
Global Tuning:  43%|████▎     | 13/30 [21:55<21:48, 76.98s/it]
Global Tuning:  47%|████▋     | 14/30 [23:11<20:24, 76.52s/it]
Global Tuning:  50%|█████     | 15/30 [24:28<19:09, 76.62s/it]
Global Tuning:  53%|█████▎    | 16/30 [25:45<17:54, 76.75s/it]
Global Tuning:  57%|█████▋    | 17/30 [27:00<16:32, 76.32s/it]
Global Tuning:  60%|██████    | 18/30 [28:15<15:10, 75.91s/it]
Global Tuning:  63%|██████▎   | 19/30 [29:28<13:46, 75.11s/it]
Global Tuning:  67%|██████▋   | 20/30 [30:43<12:30, 75.01s/it]
Global Tuning:  70%|███████   | 21/30 [31:59<11:17, 75.26s/it]
Global Tuning:  73%|███████▎  | 22/30 [33:15<10:05, 75.63s/it]
Global Tuning:  77%|███████▋  | 23/30 [34:30<08:48, 75.51s/it]
Global Tuning:  80%|████████  | 24/30 [35:46<07:33, 75.63s/it]
Global Tuning:  83%|████████▎ | 25/30 [37:00<06:15, 75.17s/it]
Global Tuning:  87%|████████▋ | 26/30 [38:15<05:00, 75.13s/it]
Global Tuning:  90%|█████████ | 27/30 [39:34<03:48, 76.08s/it]
Global Tuning:  93%|█████████▎| 28/30 [40:51<02:32, 76.38s/it]
Global Tuning:  97%|█████████▋| 29/30 [42:09<01:16, 76.80s/it]
Global Tuning: 100%|██████████| 30/30 [43:27<00:00, 77.35s/it]
Global Tuning: 100%|██████████| 30/30 [43:27<00:00, 86.93s/it]
Compiling MALA body

Global Sampling:   0%|          | 0/30 [00:00<?, ?it/s]
Global Sampling:   3%|▎         | 1/30 [01:16<36:44, 76.03s/it]
Global Sampling:   7%|▋         | 2/30 [02:31<35:25, 75.91s/it]
Global Sampling:  10%|█         | 3/30 [03:48<34:16, 76.17s/it]
Global Sampling:  13%|█▎        | 4/30 [05:05<33:10, 76.54s/it]
Global Sampling:  17%|█▋        | 5/30 [06:19<31:35, 75.83s/it]
Global Sampling:  20%|██        | 6/30 [07:36<30:22, 75.95s/it]
Global Sampling:  23%|██▎       | 7/30 [08:55<29:29, 76.95s/it]
Global Sampling:  27%|██▋       | 8/30 [10:12<28:16, 77.10s/it]
Global Sampling:  30%|███       | 9/30 [11:29<26:55, 76.92s/it]
Global Sampling:  33%|███▎      | 10/30 [12:43<25:22, 76.14s/it]
Global Sampling:  37%|███▋      | 11/30 [14:00<24:08, 76.25s/it]
Global Sampling:  40%|████      | 12/30 [15:16<22:54, 76.39s/it]
Global Sampling:  43%|████▎     | 13/30 [16:34<21:43, 76.67s/it]
Global Sampling:  47%|████▋     | 14/30 [17:50<20:24, 76.54s/it]
Global Sampling:  50%|█████     | 15/30 [19:07<19:11, 76.77s/it]
Global Sampling:  53%|█████▎    | 16/30 [20:22<17:47, 76.25s/it]
Global Sampling:  57%|█████▋    | 17/30 [21:39<16:32, 76.34s/it]
Global Sampling:  60%|██████    | 18/30 [22:55<15:17, 76.42s/it]
Global Sampling:  63%|██████▎   | 19/30 [24:15<14:11, 77.42s/it]
Global Sampling:  67%|██████▋   | 20/30 [25:32<12:51, 77.18s/it]
Global Sampling:  70%|███████   | 21/30 [26:49<11:35, 77.23s/it]
Global Sampling:  73%|███████▎  | 22/30 [28:06<10:17, 77.22s/it]
Global Sampling:  77%|███████▋  | 23/30 [29:23<08:59, 77.09s/it]
Global Sampling:  80%|████████  | 24/30 [30:39<07:41, 76.90s/it]
Global Sampling:  83%|████████▎ | 25/30 [31:58<06:26, 77.39s/it]
Global Sampling:  87%|████████▋ | 26/30 [33:19<05:14, 78.51s/it]
Global Sampling:  90%|█████████ | 27/30 [34:37<03:55, 78.46s/it]
Global Sampling:  93%|█████████▎| 28/30 [35:56<02:36, 78.35s/it]
Global Sampling:  97%|█████████▋| 29/30 [37:14<01:18, 78.46s/it]
Global Sampling: 100%|██████████| 30/30 [38:33<00:00, 78.58s/it]
Global Sampling: 100%|██████████| 30/30 [38:33<00:00, 77.12s/it]
Training summary
==========
E_sym: 36.513 +/- 4.768
L_sym: 38.369 +/- 20.641
K_sym: -196.318 +/- 74.047
K_sat: 209.310 +/- 39.327
nbreak: 0.273 +/- 0.036
n_CSE_0_u: 0.489 +/- 0.273
cs2_CSE_0: 0.533 +/- 0.288
n_CSE_1_u: 0.488 +/- 0.270
cs2_CSE_1: 0.630 +/- 0.260
n_CSE_2_u: 0.509 +/- 0.277
cs2_CSE_2: 0.603 +/- 0.267
n_CSE_3_u: 0.505 +/- 0.273
cs2_CSE_3: 0.541 +/- 0.278
n_CSE_4_u: 0.502 +/- 0.275
cs2_CSE_4: 0.524 +/- 0.279
n_CSE_5_u: 0.494 +/- 0.275
cs2_CSE_5: 0.513 +/- 0.280
n_CSE_6_u: 0.509 +/- 0.277
cs2_CSE_6: 0.504 +/- 0.279
n_CSE_7_u: 0.507 +/- 0.274
cs2_CSE_7: 0.494 +/- 0.279
cs2_CSE_8: 0.515 +/- 0.281
key: 621469968.422 +/- 4572227347.644
Log probability: -1682.340 +/- 54305.836
Local acceptance: 0.976 +/- 0.154
Global acceptance: 0.022 +/- 0.147
Max loss: 39.321, Min loss: 27.478
Production summary
==========
E_sym: 36.092 +/- 4.660
L_sym: 33.469 +/- 18.298
K_sym: -196.759 +/- 67.886
K_sat: 206.044 +/- 38.650
nbreak: 0.282 +/- 0.030
n_CSE_0_u: 0.496 +/- 0.264
cs2_CSE_0: 0.700 +/- 0.221
n_CSE_1_u: 0.453 +/- 0.253
cs2_CSE_1: 0.621 +/- 0.260
n_CSE_2_u: 0.547 +/- 0.276
cs2_CSE_2: 0.605 +/- 0.264
n_CSE_3_u: 0.473 +/- 0.260
cs2_CSE_3: 0.464 +/- 0.287
n_CSE_4_u: 0.509 +/- 0.263
cs2_CSE_4: 0.478 +/- 0.279
n_CSE_5_u: 0.462 +/- 0.259
cs2_CSE_5: 0.517 +/- 0.279
n_CSE_6_u: 0.524 +/- 0.265
cs2_CSE_6: 0.510 +/- 0.275
n_CSE_7_u: 0.539 +/- 0.266
cs2_CSE_7: 0.485 +/- 0.278
cs2_CSE_8: 0.539 +/- 0.273
key: -12344958370.650 +/- 12593359781.813
Log probability: -295.266 +/- 2.400
Local acceptance: 0.980 +/- 0.140
Global acceptance: 0.003 +/- 0.055
Sampling has been successful, now we will do some postprocessing. Sampling time: roughly 83 mins
Saving the final results
Number of samples generated in training: 630000
Number of samples generated in production: 630000
Number of samples generated: 1260000
Time taken for TOV map: 2.3987717628479004 s
The corner plot will plot the parameters: ['E_sym', 'L_sym', 'K_sym', 'K_sat', 'nbreak', 'n_CSE_0_u', 'cs2_CSE_0', 'n_CSE_1_u', 'cs2_CSE_1', 'n_CSE_2_u', 'cs2_CSE_2', 'n_CSE_3_u', 'cs2_CSE_3', 'n_CSE_4_u', 'cs2_CSE_4', 'n_CSE_5_u', 'cs2_CSE_5', 'n_CSE_6_u', 'cs2_CSE_6', 'n_CSE_7_u', 'cs2_CSE_7', 'cs2_CSE_8']
DONE entire script
DONE

JOB STATISTICS
==============
Job ID: 10895119
Cluster: snellius
User/Group: twouters2/twouters2
State: RUNNING
Nodes: 1
Cores per node: 16
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 22:50:08 core-walltime
Job Wall-clock time: 01:25:38
Memory Utilized: 0.00 MB
Memory Efficiency: 0.00% of 20.00 GB (20.00 GB/node)
WARNING: Efficiency statistics can only be obtained after the job has ended as seff tool is based on the accounting database data.
